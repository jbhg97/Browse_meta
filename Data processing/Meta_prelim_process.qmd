---
title: "Meta_prelim_test"
author: "Jamie"
format: html
editor: visual
---

# Non-lethal mammal control Meta analysis - Data clean up and processing

## Initial processing

Packages

```{r}
library(pacman)
p_load(tidyverse, readxl, here, writexl, meta, metafor, mice, miceadds, VIM, patchwork)
```

Read in

```{r}
df <- read_excel(here("/homevol/jbhg97/Browse_meta/Data processing", "Lit review data.xlsx"), sheet = "Effects (post responses)")
```

### Assign papers ID codes

I need to assign each paper an ID code that can be tied to the effects that were extracted from that paper. This will make downstream processing and modelling easier.

```{r}
df <- df %>% mutate(ref = paste(Authors,`Article Title`, `Source Title`))

df1 <- df %>% group_by(ref) %>% mutate(Paper_ID = cur_group_id()) %>% ungroup()
```

### Standardise time

Length of trial was measured in various units from years to hours. I need to standardise the units so that they can be easily compared later. I'll take a look at the breakdown of the different units and make a call as to which unit to use as standard - a priori I think months will probably be optimal.

```{r}
df1 <- df1 %>% mutate(Time_unit = as.factor(Time_unit))

summary(df1)
```

Opting for months as the best option. Need to standardise so that all the times are in months. Write a function that will convert all the different units into months and then execute that function using the Time and Time_unit data in the dataframe.

Write the function

```{r}
to_months <- function(Time_unit, Time) {
  case_when(Time_unit == is.na(Time_unit) ~ NA,
            Time_unit == "day" ~ Time / 30.417,
            Time_unit == "week" ~ Time / (30.417/7),
            Time_unit == "month" ~ Time,
            Time_unit == "year" ~ Time * 12,
            Time_unit == "hour" ~ Time / (30.417*24))
}
```

Execute

```{r}
df2 <- df1 %>% rowwise() %>% 
  mutate(Months = to_months(Time_unit, Time))

#take a look at the spread of the months data
summary(df2)
```

Split out LS means here into their own dataframe - may reinclude but will need to do sensitivity analyses.

```{r}
df_LS <- df2 %>% filter(!is.na(`Control LS mean`))
df3 <- df2 %>% filter(is.na(`Control LS mean`))
```

### Simplify moderators

#### Browsing fauna

Browsing animal data could present problems with there being too many levels. Will move to simplify this into 5 broader categories that fill similar browsing niches.

Assess the number of different levels and how many fall into each level.

```{r}
table(df3$Animal)
```

Write a function to group the levels into broader categories. Deer is a large enough group to be left as is.

```{r}
group_fauna <- function(Animal){
  case_when(Animal %in% c("Beaver", "Coypu", "Kangaroo", "Pademelon (captive)", "Wallaby", "Wallaby, pademelon, possum", "Hares", "Possum (captive)", "Hyrax", "Kangaroo, wallaby", "Rabbits", "Voles") ~ "Other",
            Animal %in% c("Deer, ELk", "Deer, sheep", "Goats", "Moose", "Reindeer", "Sheep", "Wild boar, sheep, goat", "Cattle", "Deer, elk", "Deer, pigs", "Elk", "Ungulates", "Llama") ~ "Other ungulates", 
            Animal == "Deer" ~ "Deer",
            Animal %in% c("Ungulates, hares", "Beaver, rabbit, deer", "Deer, hare", "Deer, voles, rabbits", "Kangaroo, goat, rabbits", "Moose, hares", "Ungulates, hares, rabbits", "Wallaby, deer", "Wallaby, kangaroo, deer, rabbit") ~ "Mixed",
            Animal == "Not specified" ~ "Not specified")
}
```

Execute and then check

```{r}
df4 <- df3 %>% mutate(Animal = group_fauna(Animal))

table(df4$Animal)
```

#### Control methods

Check the split of control methods - do repellents need to be grouped? Different fencing strategies?

```{r}
table(df4$`Control method`)
```

Can group some of these - cover crop and plant refuge. fertiliser, nursery, genetic resistance (stock manipulation), repellents, other fencing (maintain the split between exclusion and other fencing strategies as they are not the same).

```{r}
group_method <- function(Method){
  case_when(Method %in% c("Cover crop", "Plant refuge") ~ "Neighbour effects",
            Method %in% c("Electric fence", "Fencing") ~ "Other fencing",
            Method %in% c("Fertiliser treatment", "Genetic resistance", "Nursery treatment", "Silvicultural treatment") ~ "Stock manipulation",
            Method %in% c("Repellent (absorption)", "Repellent (hanging)", "Repellent (on ground)", "Repellent (on-browse)") ~ "Repellents",
            TRUE ~ Method)
}
```

Execute and check

```{r}
df5 <- df4 %>% mutate(`Control method` = group_method(`Control method`))

table(df5$`Control method`)
```

## Conversions and transformations

Separate data into SMD and log odds effects. Different transformations for each type of data

```{r}
df_SMD <- df5 %>% filter(`Effect type` == "SMD")
df_lo <- df5 %>% filter (`Effect type` == "Log odds")
```

### SMD Mean Proportion data

Proportion style data measured across multiple plots and therefore producing a mean can be problematic. Need to process this so that first, the units are the same ie all proportions. Second, transform it to put the data on to an unbounded scale - see Harrison et al 2021 biorev paper.

```{r}
#Identify problem rows
unique(df_SMD$`Var. measured`)
```

Checked this df in excel and identified combinations of variable and unit that will need to be converted as the measurements are bounded 0-100/0-1 based on percent/proportion presented.

I need to filter out the problem rows. Apply the transformations and then join them back to the data.

```{r}
#filter - concatenate variable and unit and then filter out problem combinations

df_SMD_trans <- df_SMD %>% mutate(Var_unit = paste(`Var. measured`, Units, sep = "_"))

df_SMD_trans_y <- df_SMD_trans %>% filter(Var_unit == "Cover_Percent"|Var_unit == "Browsing_Percent"|Var_unit == "Plants browsed_Percent"|Var_unit == "Survival_Percent"|Var_unit == "Prop. bottom consumed_Percent"|Var_unit == "Prop. top consumed_Percent"|Var_unit == "Stems damaged_Proportion"|Var_unit == "Mortality_Percent"|Var_unit == "Plants browsed_Proportion"|Var_unit == "Survival_Proportion"|Var_unit == "Browse intensity_Percent"|Var_unit == "Lateral shoot browsing_Percent"|Var_unit == "Leader browsing_Percent"|Var_unit == "Establishment_Percent"|Var_unit == "Browsing damage_Percent"|Var_unit == "Deformed base_Percent"|Var_unit == "Deformed stem_Percent"|Var_unit == "Multi stem plants_Percent"|Var_unit == "Foliage removed_Percent"|Var_unit == "Consumption_Percent/day"|Var_unit == "Branches browsed_Percent"|Var_unit == "Juveniles browsed_Proportion"|Var_unit == "Suckers browsed_Proportion"|Var_unit == "Plants browsed_Probability"|Var_unit == "Leader browsing_Probability"|Var_unit == "Shoots browsed_Percent"|Var_unit == "Browsing_score_Score")

df_SMD_trans_n <- df_SMD_trans %>% filter(!(Var_unit == "Cover_Percent"|Var_unit == "Browsing_Percent"|Var_unit == "Plants browsed_Percent"|Var_unit == "Survival_Percent"|Var_unit == "Prop. bottom consumed_Percent"|Var_unit == "Prop. top consumed_Percent"|Var_unit == "Stems damaged_Proportion"|Var_unit == "Mortality_Percent"|Var_unit == "Plants browsed_Proportion"|Var_unit == "Survival_Proportion"|Var_unit == "Browse intensity_Percent"|Var_unit == "Lateral shoot browsing_Percent"|Var_unit == "Leader browsing_Percent"|Var_unit == "Establishment_Percent"|Var_unit == "Browsing damage_Percent"|Var_unit == "Deformed base_Percent"|Var_unit == "Deformed stem_Percent"|Var_unit == "Multi stem plants_Percent"|Var_unit == "Foliage removed_Percent"|Var_unit == "Consumption_Percent/day"|Var_unit == "Branches browsed_Percent"|Var_unit == "Juveniles browsed_Proportion"|Var_unit == "Suckers browsed_Proportion"|Var_unit == "Plants browsed_Probability"|Var_unit == "Leader browsing_Probability"|Var_unit == "Shoots browsed_Percent"|Var_unit == "Browsing_score_Score"))
```

Only convert and transform the means here - will convert and transform the errors after the missing cases have been cut out and the errors converted to SD.

```{r}
#Convert percentages to proportions - probability is the same as proportion so need to convert. Will also add a column to the dataframe that shows these data were transformed.

#write a function for converting based on units

pct_to_prop <- function(Unit, Value){
  case_when(Unit == "Percent" ~ Value/100,
            Unit == "Percent/day" ~ Value/100,
            Unit == "Probability" ~ Value,
            Unit == "Proportion" ~ Value)
}

df_SMD_trans_y1 <- df_SMD_trans_y %>% 
  mutate(Trans = "Y",
         Con_trans_mean = pct_to_prop(Units, `Control mean`),
         Trt_trans_mean = pct_to_prop(Units, `Treatment mean`))

#write a function to transform the means

logit_trans_means <- function(Value){
  trans <- log(Value/(1 - Value))
  return(trans)
}

df_SMD_trans_y2 <- df_SMD_trans_y1 %>% mutate(Con_trans_mean = logit_trans_means(Con_trans_mean),
                                              Trt_trans_mean = logit_trans_means(Trt_trans_mean))
```

Put the data back together for error conversion

```{r}
df_SMD_trans_n1 <- df_SMD_trans_n %>% mutate(Trans = "N")
df_lo <- df_lo %>% mutate(Trans = "N") #these add an indicator that can be used later for simple sensitivity analyses

df_SMD <- bind_rows(df_SMD_trans_n1, df_SMD_trans_y2)
```

### SMD Convert errors to SD

The SMD effect size formula uses standard deviation to generate effect sizes. The errors in my data are in various formats and therefore need to be standardised. Possible to do something like with the time where I have 2 columns, one with the numeric value and one with the error type for treatment and control. Then I can write a function that will do the different conversions based on the type of error.

Split data into complete cases and errors missing

```{r}
df_SMD_miss <- df_SMD %>%
  filter(if_all(c(`Control SD`, `Control SE`, `Control CV`, `Control 95CI(lower:upper)`), ~ all(is.na(.))))

df_SMD1 <- df_SMD %>% 
  filter(if_any(c(`Control SD`, `Control SE`, `Control CV`, `Control 95CI(lower:upper)`), ~ !is.na(.)))
```

Take complete cases and rearrange error columns into - error value and error type

```{r}
df_SMD1 <- df_SMD1 %>% mutate(`Control SD` = as.numeric(`Control SD`)) %>% 
  rownames_to_column() %>% 
  pivot_longer(cols = c(`Control SD`, `Control SE`, `Control CV`, `Control 95CI(lower:upper)`),
               names_to = "Control_Error_type",
               values_to = "Control_Error") %>% 
  drop_na(Control_Error)

df_SMD2 <- df_SMD1 %>% select(-rowname) %>% 
  rownames_to_column() %>% 
  pivot_longer(cols = c(`Treatment SD`, `Treatment SE`, `Treatment CV`, `Treatment 95CI`),
               names_to = "Treatment_Error_type",
               values_to = "Treatment_Error") %>% 
  drop_na(Treatment_Error)
```

Tidy up the strings so they don't have control/treatment at the start of the cell entry in the dataframe.

```{r}
df_SMD3<- df_SMD2 %>%
  mutate(Control_Error_type = case_when(
    Control_Error_type == "Control SE" ~ "SE",
    Control_Error_type == "Control SD" ~ "SD",
    Control_Error_type == "Control CV" ~ "CV",
    Control_Error_type == "Control 95CI(lower:upper)" ~ "95CI",
    TRUE ~ Control_Error_type),
    Treatment_Error_type = case_when(
      Treatment_Error_type == "Treatment SE" ~ "SE",
      Treatment_Error_type == "Treatment SD" ~ "SD",
      Treatment_Error_type == "Treatment CV" ~ "CV",
      Treatment_Error_type == "Treatment 95CI" ~ "95CI",
      TRUE ~ Treatment_Error_type))
```

Write a function for the conversion to SD.

```{r}
to_SD <- function(Error_type, Value, Sample_size, Mean) {
  case_when(
    Error_type == "SE" ~ Value * sqrt(Sample_size),
    Error_type == "SD" ~ Value,
    Error_type == "CV" ~ Value * Mean
   # Error_type == "95CI" ~ Value #issue here as the only paper that I have errors as 95CI also used LS means - cross this bridge if LS means become included
  )
}
```

Execute

```{r}
df_SMD4 <- df_SMD3 %>% 
  mutate(Con_SD = to_SD(Control_Error_type, Control_Error, `Control sample size`, `Control mean`),
         Trt_SD = to_SD(Treatment_Error_type, Treatment_Error, `Treatment sample`, `Treatment mean`))
```

### SMD Trim up

The output dataframes that I will use for analysis do not need to contain columns that were relevant for data extraction and identification.

I'll rename some column headings for greater clarity and then trim down the dataframe to the necessary columns.

```{r}
df_SMD5 <- df_SMD4 %>% 
  rename(Method = `Control method`, Type = `Method sub-type`, Plant_genus = `Plant genus`, Scale = `Scale (plant, coupe, landscape)`, Variable = `Var. measured`, Confounding = `Confounding factors`, Effect_type = `Effect type`, Con_N = `Control sample size`, Con_mean = `Control mean`, Trt_N = `Treatment sample`, Trt_mean = `Treatment mean`, Plot_size = `Plot_size(m2)`)
```

```{r}
df_SMD6 <- df_SMD5 %>% 
  select(Paper_ID, Country, Method, Type, Animal, Plant_genus, Scale, Plot_size, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Con_SD, Trt_N, Trt_mean, Trt_SD, Trans, Con_trans_mean, Trt_trans_mean)
```

### SMD Convert and transform errors

Now I will transform the SDs for the SMD proportion data as mentioned earlier - use Trans = "Y" to quickly identify the rows for which this is relevant and then apply the functions written earlier.

```{r}
# write function for SD
logit_trans_SD <- function(SD, prop, ID){
  if_else(ID == "Y", sqrt(((SD^2)*(1/prop)) + (1/(1-(prop^2)))), NA)
}

# first convert instances where SD is still percentage rather than proportion

df_SMD_sdtrans <- df_SMD6 %>% mutate(Con_trans_SD = pct_to_prop(Units, Con_SD),
                                     Trt_trans_SD = pct_to_prop(Units, Trt_SD))

#then execute logit function

df_SMD_sdtrans1 <- df_SMD_sdtrans %>% mutate(Con_trans_SD = logit_trans_SD(Con_SD, Con_mean, Trans), 
                                             Trt_trans_SD = logit_trans_SD(Trt_SD, Trt_mean, Trans))
```

Write this dataframe out as a record of the Complete case data showing both raw means and sds and transformed means and sds

```{r}
write_xlsx(df_SMD_sdtrans1, "/homevol/jbhg/Browse_meta/Data processing/SMD_compcase_raw and transform.xlsx")
```

### SMD Quality control 1

Need to impute the errors for cases where no error was reported - this is handled later.

I need to assess the quality of the data that will be used for the imputation and further downstream analyses. Instances where the control/treatment SD is 0 will not be suitable for generating SMD effect sizes as they cause an effect size of infinity. Moreover, instances where either sample size is 1 will result in an SD = 0 creating the same problem. I will remove these cases and then conduct a modified Geary's test (Lajeunesse 2015, Nakagawa et al 2023).

Remove problematic data

```{r}
# Filter out instances where control/treatment SD = 0
df_SMD7 <- df_SMD_sdtrans1 %>% 
  filter(Con_SD != 0 & Trt_SD != 0)

# Filter out instances where control/treatment N = 1 
df_SMD8 <-  df_SMD7 %>% 
  filter(Con_N != 1 & Trt_N != 1)

#filter out transformed data that was problematic ie produced an infinite value
df_SMD9 <- df_SMD8 %>% 
  filter(Con_trans_mean != Inf | is.na(Con_trans_mean)) %>% filter(Trt_trans_mean != Inf | is.na(Trt_trans_mean)) %>%
  filter(Con_trans_SD != Inf | is.na(Con_trans_SD)) %>% filter(Trt_trans_SD != Inf | is.na(Trt_trans_SD))
```

### LO Trim up

Clean up the dataframe and rename some columns for easier downstream processing.

```{r}
df_lo1 <- df_lo %>% 
  rename(Method = `Control method`, Type = `Method sub-type`, Plant_genus = `Plant genus`, Scale = `Scale (plant, coupe, landscape)`, Variable = `Var. measured`, Confounding = `Confounding factors`, Effect_type = `Effect type`, Con_N = `Control sample size`, Con_mean = `Control mean`, Trt_N = `Treatment sample`, Trt_mean = `Treatment mean`, Plot_size = `Plot_size(m2)`)
```

```{r}
df_lo2 <- df_lo1 %>% 
  select(Paper_ID, Country, Method, Type, Animal, Plant_genus, Scale, Plot_size, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Trt_N, Trt_mean, Trans)
```

### LO Count data

The binomial data I have eg survival or browsed/not browsed has been measured with various units: percent, proportion, probability, count etc. I need to get all of the log odds effects back to count data so that I can generate effect sizes. I need to write a function to convert the values based on the units. Then I can generate the effect sizes.

What units need to be accounted for? Make any edits for entry errors

```{r}
df_lo_units <- df_lo2 %>% 
  distinct(Units)

df_lo2 <-  df_lo2 %>% 
  mutate(Units = ifelse(Units == "count", "Count", Units))
```

Write the function

```{r}
to_count <- function(Unit, Value, Sample_size) {
  counts <- case_when(
    Unit == "Count" ~ Value,
    Unit == "Percent" ~ (Sample_size/100) * Value,
    Unit ==  "Proportion" ~ Sample_size * Value,
    Unit ==  "Probability" ~ Sample_size * Value)
  
  output <- round(counts, 0) #count data has to be whole number - survival etc can't be half a plant survived
  
  return(output)
} 
```

Execute and trim

```{r}
df_lo3 <- df_lo2 %>% 
  mutate(Con_count = to_count(Units, Con_mean, Con_N),
         Trt_count = to_count(Units, Trt_mean, Trt_N)) %>% 
  select(Paper_ID, Country, Method, Type, Animal, Plant_genus, Scale, Plot_size, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_count, Trt_N, Trt_count, Trans)
```

## Imputation of missing SD and Plot Size

### Rejoin data

I need to rebind the missing cases to the full cases so SD can be imputed but also the Log odds so any LO effects with missing plot size can be imputed as well. First, I need to make sure the missing cases have the same structure.

```{r}
df_SMD_miss1 <- df_SMD_miss %>%
  rename(Method = `Control method`, Type = `Method sub-type`, Plant_genus = `Plant genus`, Scale = `Scale (plant, coupe, landscape)`, Variable = `Var. measured`, Confounding = `Confounding factors`, Effect_type = `Effect type`, Con_N = `Control sample size`, Con_mean = `Control mean`, Trt_N = `Treatment sample`, Trt_mean = `Treatment mean`, Plot_size = `Plot_size(m2)`) %>% 
  mutate(Con_SD = NA,
         Trt_SD = NA) %>% 
  select(Paper_ID, Country, Method, Type, Animal, Plant_genus, Scale, Plot_size, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Con_SD, Trt_N, Trt_mean, Trt_SD, Trans, Con_trans_mean, Trt_trans_mean)
  
```

Join SMD effects together

```{r}
df_SMD_all <- bind_rows(df_SMD9, df_SMD_miss1)
```

Ensure correct transformed means and SDs are being used

```{r}
df_SMD_all1 <- df_SMD_all %>% mutate(Con_mean = if_else(Trans == "N", Con_mean, Con_trans_mean),
                              Con_SD = if_else(Trans == "N", Con_SD, Con_trans_SD),
                              Trt_mean = if_else(Trans == "N", Trt_mean, Trt_trans_mean),
                              Trt_SD = if_else(Trans == "N", Trt_SD, Trt_trans_SD)) %>% 
  select(Paper_ID, Country, Method, Type, Animal, Plant_genus, Scale, Plot_size, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Con_SD, Trt_N, Trt_mean, Trt_SD, Trans)
```

#### Quality control

There are some instances where the logit transformation of the proportion data has generated an infinite mean. These will need to be removed.

```{r}
df_SMD_all2 <- df_SMD_all1 %>% 
  filter(Con_mean != -Inf | is.na(Con_mean)) %>% filter(Trt_mean != Inf | is.na(Trt_mean))
```

#### Add imputation indicator

```{r}
df_SMD_all3 <- df_SMD_all2 %>% mutate(Imputed = if_else(is.na(Con_SD & Plot_size), "Y", "N"))

df_lo4 <- df_lo3 %>% mutate(Imputed = if_else(is.na(Plot_size), "Y", "N"))
```

#### Join data

```{r}
df_all <- bind_rows(df_SMD_all3, df_lo4)
```

### Build the predictor matrix

```{r}
# Make predictor matrix which defines which variables will be used to impute. 
predmatHg <- make.predictorMatrix(df_all)
predmatHg[, c(2:5,10:15,22:25)] <- 0
predmatHg[c(1:17,19,20,22:25), ] <- 0 # these rows deal with SD imputation - using PaperID, plant genus, scale, plot size, time and N, mean and SD 
predmatHg[8, c(1,7,16,19)] <- 1 # this imputes plot size using PaperID, scale and N (smaller plots = higher N and vice versa, Spake et al. (2020))
```

### Multivariate imputation with mice

```{r}
impData_all <- mice(df_all, m=20, maxit=50, meth = c("","","","","","","","pmm","","","","","","","","","","pmm","","","pmm","","","",""), pred = predmatHg, seed=500)
summary(impData_all)
```

```{r}
# Looking at these there are 2 papers with very large means and errors that are making the plots difficult to interpret (Kuiters et al and Leonardsson et al) - these data are accurate, add limits to view the bulk of the imputed data.
mice::xyplot(impData_all, Con_SD ~ Con_mean)
mice::xyplot(impData_all, Trt_SD ~ Trt_mean, xlim = c(0,20000), ylim = c(0,20000))

# need to work out a plot for the plot size data so I can see the quality of the imputation

# Get all imputed datasets
df_imp_all <- complete(impData_all, "all")
```

## Complete case effects

Need to split the data back into complete case and imputed and then into SMD and log odds

```{r}
df_comp_SMD <- df_all %>% filter(Imputed == "N") %>% filter(Effect_type == "SMD")
df_comp_lo <- df_all %>% filter(Imputed == "N") %>% filter(Effect_type == "Log odds")
df_imp_SMD <- lapply(df_imp_all, function(df){ df %>% filter(Imputed == "Y") %>% filter(Effect_type == "SMD")})
df_imp_lo <- lapply(df_imp_all, function(df){ df %>% filter(Imputed == "Y") %>% filter(Effect_type == "Log odds")}) 
```

### SMD effects

#### Generation

Take the complete case data and generate SMD effect sizes using metafor - escalc()

```{r}
df_comp_SMD1 <- df_comp_SMD %>% mutate(Effect_size = escalc("SMD", m1i = Trt_mean, sd1i = Trt_SD, n1i = Trt_N, m2i = Con_mean, sd2i = Con_SD, n2i = Con_N, data = df_comp_SMD, var.names = c("Effect_size", "Sampling_variance"), append = FALSE))
```

#### Coining

What variables were measured?

```{r}
df_SMD_var <- df_SMD_all %>% distinct(Variable)
```

Write the function

```{r}
coin_SMD <- function(Variable, Value) {
  case_when(Variable == "sqrt Height removed" | Variable == "Bark wounds/tree" | Variable == "Branches browsed" | 
            Variable == "Browse intensity" | Variable == "Browsing" | Variable == "Browsing damage" | Variable == "Consumption" |
            Variable == "Damage score" | Variable == "Deformed base" | Variable == "Deformed stem" | Variable == "Foliage removed" |
            Variable == "Height removed" | Variable == "Height to 1st branch" | Variable == "Juveniles browsed" |
            Variable == "Lateral shoot browsing" | Variable == "Leader browsing" | Variable == "Leaves eaten" | 
            Variable == "log(twigs|stems browsed)" | Variable == "Mass of browsed seedlings" | Variable == "Mortality" |
            Variable == "Multi stem plants" | Variable == "No. of leaders" | Variable == "Plants browsed" |
            Variable == "Prop. bottom consumed" | Variable == "Prop. top consumed" | Variable == "Shoots browsed" |
            Variable == "Shoots eaten/plant" | Variable == "Stems damaged" | Variable == "Suckers browsed" |
            Variable == "Twigs removed/tree" | Variable == "Twigs/tree browsed" | Variable == "Twigs|Stems browsed" |
            Variable == "Browsing_score" ~ Value * -1,
            TRUE  ~ Value)
}
```

Execute - need to unnest the Effect_size columns first (escalc class objects don't work for these functions)

```{r}
df_comp_SMD2 <- df_comp_SMD1 %>% unnest_wider(Effect_size) %>% mutate(Effect_size = coin_SMD(Variable, Effect_size))
```

#### Write out

```{r}
write_xlsx(df_comp_SMD2, "/homevol/jbhg/Browse_meta/Data processing/SMD_compcase_es.xlsx")
```

### Log odds effects

#### Generation

Use metafor escalc() to calculate the log odds ratio effect sizes.

```{r}
df_comp_lo1 <- df_comp_lo %>% mutate(Effect_size = escalc(measure = "OR", ai = Trt_count, n1i = Trt_N, ci = Con_count, n2i = Con_N, data = df_comp_lo, var.names = c("Effect_size", "Sampling_variance"), append = FALSE))
```

#### Coining

What variables were measured?

```{r}
df_lo_var <- df_lo3 %>% distinct(Variable)
```

Write the function

```{r}
coin_lo <- function(Variable, Value) {
  case_when(Variable == "Mortality" | Variable == "Plants damaged" | Variable == "Plants browsed" | Variable == "Leader browsing" |
            Variable == "Lateral browsing" | Variable == "Bud damage" | Variable == "Leaf damage" | Variable == "Stems harvested" 
            ~ Value * -1,
            TRUE ~ Value)
}
```

Execute

```{r}
df_comp_lo2 <- df_comp_lo1 %>% unnest_wider(Effect_size) %>% mutate(Effect_size = coin_lo(Variable, Effect_size))
```

#### Write out

```{r}
write_xlsx(df_comp_lo2, "/homevol/jbhg/Browse_meta/Data processing/LO_compcase_es.xlsx")
```

## Imputed case effects

### SMD effects

#### Generation

```{r}
df_imp_SMD1 <- lapply(df_imp_SMD, function(df){
  df %>% mutate(Effect_size = escalc("SMD", m1i = Trt_mean, sd1i = Trt_SD, n1i = Trt_N, m2i = Con_mean, sd2i = Con_SD, n2i = Con_N, data = df, var.names = c("Effect_size", "Sampling_variance"), append = FALSE))
 })
```

#### Coining

```{r}
df_imp_SMD2 <- lapply(df_imp_SMD1, function(df){
  df %>% unnest_wider(Effect_size) %>% mutate(Effect_size = coin_SMD(Variable, Effect_size))
})
```

#### Write out

```{r}
saveRDS(df_imp_SMD2, "/homevol/jbhg97/Browse_meta/Data processing/SMD_impcase_es")
```

### Log odds effects

#### Generation

```{r}
df_imp_lo1 <- lapply(df_imp_lo, function(df){
  df %>% mutate(Effect_size <- escalc(measure = "OR", ai = Trt_count, n1i = Trt_N, ci = Con_count, n2i = Con_N, data = df, var.names = c("Effect_size", "Sampling_variance"), append = FALSE)) })
```

#### Coining

```{r}
df_imp_lo2 <- lapply(df_imp_lo1, function(df){
  df %>% mutate(Effect_size = coin_lo(Variable, Effect_size))
})
```

#### Write out

```{r}
saveRDS(df_imp_lo2, "/homevol/jbhg97/Browse_meta/Data processing/LO_impcase_es")
```

## All effects comparison

To compare Log odds ratio effects with standardised mean difference (Hedges' g) the effect size and sampling variances need to be converted.

### Log odds to SMD functions

Write the functions for converting from Log odds to SMD - using formulas from Borenstein chapter in Cooper et al. (2019) Handbook of meta-analysis

```{r}
lo_to_SMD_ES <- function(Lo_es){
  SMD_es <- (Lo_es*sqrt(3)) / pi 
  
  return(SMD_es)
}

lo_to_SMD_var <- function(Lo_var){
  SMD_var <- (Lo_var*3) / (pi^2) 
  
  return(SMD_var)
}
```

### Complete case

Execute those functions.

```{r}
df_comp_lo_conv <- df_comp_lo2 %>% mutate(Effect_size = lo_to_SMD_ES(Effect_size),
                                          Sampling_variance = lo_to_SMD_var(Sampling_variance))
```

### Imputed case

```{r}
df_imp_lo_conv <- lapply(df_imp_lo2, function(df) {
  df %>% mutate(Effect_size = lo_to_SMD_ES(Effect_size),
                Sampling_variance = lo_to_SMD_var(Sampling_variance))
})
```

### Rejoin

Join converted log odds data with SMD data. The imputed case data was split after imputation, therefore I need to ensure that dataframe 1 from the SMD list is rejoined to dataframe 1 of the Log odds list.

```{r}
df_comp_all <- bind_rows(df_comp_SMD2, df_comp_lo_conv) 
  
df_imp_all <- list()
for (i in seq_along(df_imp_SMD2)) { df_imp_all[[i]] <- bind_rows(df_imp_SMD2[[i]], df_imp_lo2[[i]]) }

df_imp_all1 <- list()
for (i in seq_along(df_imp_all)) { df_imp_all1[[i]] <- bind_rows(df_imp_all[[i]], df_comp_all) }
```

```{r}
write_xlsx(df_comp_all, "/homevol/jbhg97/Browse_meta/Data processing/All_compcase_es.xlsx")
saveRDS(df_imp_all1, "/homevol/jbhg97/Browse_meta/Data processing/All_impcase_es")
```

## Divide data for analysis

```{r}
df_comp_plant <- df_comp_all %>% filter(Scale == "Plant")
write_xlsx(df_comp_plant, "/homevol/jbhg97/Browse_meta/Analysis/Comp_all_plant.xlsx")
  
df_comp_coupe <- df_comp_all %>% filter(Scale == "Coupe")
write_xlsx(df_comp_coupe, "/homevol/jbhg97/Browse_meta/Analysis/Comp_all_coupe.xlsx")
  
df_comp_lscape <- df_comp_all %>% filter(Scale == "Landscape")
write_xlsx(df_comp_lscape, "/homevol/jbhg97/Browse_meta/Analysis/Comp_all_lscape.xlsx")
```

```{r}
df_imp_plant <- lapply(df_imp_all1, function(df){ df %>% filter(Scale == "Plant") })
saveRDS(df_imp_plant, "/homevol/jbhg97/Browse_meta/Analysis/Imp_all_plant")
  
df_imp_coupe <- lapply(df_imp_all1, function(df){ df %>% filter(Scale == "Coupe") })
saveRDS(df_imp_coupe, "/homevol/jbhg97/Browse_meta/Analysis/Imp_all_coupe")
  
df_imp_lscape <- lapply(df_imp_all1, function(df){ df %>% filter(Scale == "Landscape") })
saveRDS(df_imp_lscape, "/homevol/jbhg97/Browse_meta/Analysis/Imp_all_lscape")
```

```{r}
rm(list = ls())
```
