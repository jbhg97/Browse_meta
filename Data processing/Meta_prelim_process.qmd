---
title: "Mammal browsing control meta-analysis data processing"
author: "Jamie"
format: html
editor: visual
---

# Non-lethal mammal control Meta analysis - Data clean up and processing

## Initial processing

Packages

```{r}
library(pacman)
p_load(tidyverse, readxl, here, writexl, meta, metafor, mice, miceadds, VIM, patchwork)
```

Read in

```{r}
df <- read_excel(here("/homevol/jbhg/Browse_meta/Data processing", "Lit review data.xlsx"), sheet = "Effects (post responses)")
```

### Assign papers ID codes

I need to assign each paper an ID code that can be tied to the effects that were extracted from that paper. This will make downstream processing and modelling easier.

```{r}
df <- df %>% mutate(ref = paste(Authors,`Article Title`, `Source Title`))

df1 <- df %>% group_by(ref) %>% mutate(Paper_ID = cur_group_id()) %>% ungroup()
```

### Standardise time

Length of trial was measured in various units from years to hours. I need to standardise the units so that they can be easily compared later. I'll take a look at the breakdown of the different units and make a call as to which unit to use as standard - a priori I think months will probably be optimal.

```{r}
df1 <- df1 %>% mutate(Time_unit = as.factor(Time_unit))

summary(df1)
```

Opting for months as the best option. Need to standardise so that all the times are in months. Write a function that will convert all the different units into months and then execute that function using the Time and Time_unit data in the dataframe.

Write the function

```{r}
to_months <- function(Time_unit, Time) {
  case_when(Time_unit == is.na(Time_unit) ~ NA,
            Time_unit == "day" ~ Time / 30.417,
            Time_unit == "week" ~ Time / (30.417/7),
            Time_unit == "month" ~ Time,
            Time_unit == "year" ~ Time * 12,
            Time_unit == "hour" ~ Time / (30.417*24))
}
```

Execute

```{r}
df2 <- df1 %>% rowwise() %>% 
  mutate(Months = to_months(Time_unit, Time))

#take a look at the spread of the months data
summary(df2)
```

Split out LS means here into their own dataframe - may reinclude but will need to do sensitivity analyses.

```{r}
df_LS <- df2 %>% filter(!is.na(`Control LS mean`))
df3 <- df2 %>% filter(is.na(`Control LS mean`))
```

## SMD effects

Separate data into SMD and log odds effects.

```{r}
df_SMD <- df3 %>% filter(`Effect type` == "SMD")
df_lo <- df3 %>% filter (`Effect type` == "Log odds")
```

### Conversions and transformations

#### Mean Proportion data

Proportion style data measured across multiple plots and therefore producing a mean can be problematic. Need to process this so that first, the units are the same ie all proportions. Second, transform it to put the data on to an unbounded scale - see Harrison et al 2021 biorev paper.

```{r}
#Identify problem rows
unique(df_SMD$`Var. measured`)
```

Checked this df in excel and identified combinations of variable and unit that will need to be converted as the measurements are bounded 0-100/0-1 based on percent/proportion presented.

I need to filter out the problem rows. Apply the transformations and then join them back to the data.

```{r}
#filter - concatenate variable and unit and then filter out problem combinations

df_SMD_trans <- df_SMD %>% mutate(Var_unit = paste(`Var. measured`, Units, sep = "_"))

df_SMD_trans_y <- df_SMD_trans %>% filter(Var_unit == "Cover_Percent"|Var_unit == "Browsing_Percent"|Var_unit == "Plants browsed_Percent"|Var_unit == "Survival_Percent"|Var_unit == "Prop. bottom consumed_Percent"|Var_unit == "Prop. top consumed_Percent"|Var_unit == "Stems damaged_Proportion"|Var_unit == "Mortality_Percent"|Var_unit == "Plants browsed_Proportion"|Var_unit == "Survival_Proportion"|Var_unit == "Browse intensity_Percent"|Var_unit == "Lateral shoot browsing_Percent"|Var_unit == "Leader browsing_Percent"|Var_unit == "Establishment_Percent"|Var_unit == "Browsing damage_Percent"|Var_unit == "Deformed base_Percent"|Var_unit == "Deformed stem_Percent"|Var_unit == "Multi stem plants_Percent"|Var_unit == "Foliage removed_Percent"|Var_unit == "Consumption_Percent/day"|Var_unit == "Branches browsed_Percent"|Var_unit == "Juveniles browsed_Proportion"|Var_unit == "Suckers browsed_Proportion"|Var_unit == "Plants browsed_Probability"|Var_unit == "Leader browsing_Probability"|Var_unit == "Shoots browsed_Percent")

df_SMD_trans_n <- df_SMD_trans %>% filter(!(Var_unit == "Cover_Percent"|Var_unit == "Browsing_Percent"|Var_unit == "Plants browsed_Percent"|Var_unit == "Survival_Percent"|Var_unit == "Prop. bottom consumed_Percent"|Var_unit == "Prop. top consumed_Percent"|Var_unit == "Stems damaged_Proportion"|Var_unit == "Mortality_Percent"|Var_unit == "Plants browsed_Proportion"|Var_unit == "Survival_Proportion"|Var_unit == "Browse intensity_Percent"|Var_unit == "Lateral shoot browsing_Percent"|Var_unit == "Leader browsing_Percent"|Var_unit == "Establishment_Percent"|Var_unit == "Browsing damage_Percent"|Var_unit == "Deformed base_Percent"|Var_unit == "Deformed stem_Percent"|Var_unit == "Multi stem plants_Percent"|Var_unit == "Foliage removed_Percent"|Var_unit == "Consumption_Percent/day"|Var_unit == "Branches browsed_Percent"|Var_unit == "Juveniles browsed_Proportion"|Var_unit == "Suckers browsed_Proportion"|Var_unit == "Plants browsed_Probability"|Var_unit == "Leader browsing_Probability"|Var_unit == "Shoots browsed_Percent"))
```

Only convert and transform the means here - will convert and transform the errors after the missing cases have been cut out and the errors converted to SD.

```{r}
#Convert percentages to proportions - probability is the same as proportion so need to convert. Will also add a column to the dataframe that shows these data were transformed.

#write a function for converting based on units

pct_to_prop <- function(Unit, Value){
  case_when(Unit == "Percent" ~ Value/100,
            Unit == "Percent/day" ~ Value/100,
            Unit == "Probability" ~ Value,
            Unit == "Proportion" ~ Value)
}

df_SMD_trans_y1 <- df_SMD_trans_y %>% 
  mutate(Trans = "Y",
         Con_trans_mean = pct_to_prop(Units, `Control mean`),
         Trt_trans_mean = pct_to_prop(Units, `Treatment mean`))

#write a function to transform the means

logit_trans_means <- function(Value){
  trans <- log(Value/(1 - Value))
  return(trans)
}

df_SMD_trans_y2 <- df_SMD_trans_y1 %>% mutate(Con_trans_mean = logit_trans_means(Con_trans_mean),
                                              Trt_trans_mean = logit_trans_means(Trt_trans_mean))
```

Put the data back together for error conversion

```{r}
df_SMD <- bind_rows(df_SMD_trans_n, df_SMD_trans_y2)
```

#### Convert errors to SD

The SMD effect size formula uses standard deviation to generate effect sizes. The errors in my data are in various formats and therefore need to be standardised. Possible to do something like with the time where I have 2 columns, one with the numeric value and one with the error type for treatment and control. Then I can write a function that will do the different conversions based on the type of error.

Need to impute some errors before conversions so there are no NAs in the data set.

Split data into complete cases and errors missing

```{r}
df_SMD_miss <- df_SMD %>%
  filter(if_all(c(`Control SD`, `Control SE`, `Control CV`, `Control 95CI(lower:upper)`), ~ all(is.na(.))))

df_SMD <- df_SMD %>% 
  filter(if_any(c(`Control SD`, `Control SE`, `Control CV`, `Control 95CI(lower:upper)`), ~ !is.na(.)))
```

Take complete cases and rearrange error columns into - error value and error type

```{r}
df_SMD1 <- df_SMD %>% mutate(`Control SD` = as.numeric(`Control SD`)) %>% 
  rownames_to_column() %>% 
  pivot_longer(cols = c(`Control SD`, `Control SE`, `Control CV`, `Control 95CI(lower:upper)`),
               names_to = "Control_Error_type",
               values_to = "Control_Error") %>% 
  drop_na(Control_Error)

df_SMD2 <- df_SMD1 %>% select(-rowname) %>% 
  rownames_to_column() %>% 
  pivot_longer(cols = c(`Treatment SD`, `Treatment SE`, `Treatment CV`, `Treatment 95CI`),
               names_to = "Treatment_Error_type",
               values_to = "Treatment_Error") %>% 
  drop_na(Treatment_Error)
```

Tidy up the strings so they don't have control/treatment at the start of the cell entry in the dataframe.

```{r}
df_SMD3<- df_SMD2 %>%
  mutate(Control_Error_type = case_when(
    Control_Error_type == "Control SE" ~ "SE",
    Control_Error_type == "Control SD" ~ "SD",
    Control_Error_type == "Control CV" ~ "CV",
    Control_Error_type == "Control 95CI(lower:upper)" ~ "95CI",
    TRUE ~ Control_Error_type),
    Treatment_Error_type = case_when(
      Treatment_Error_type == "Treatment SE" ~ "SE",
      Treatment_Error_type == "Treatment SD" ~ "SD",
      Treatment_Error_type == "Treatment CV" ~ "CV",
      Treatment_Error_type == "Treatment 95CI" ~ "95CI",
      TRUE ~ Treatment_Error_type))
```

Write a function for the conversion to SD.

```{r}
to_SD <- function(Error_type, Value, Sample_size, Mean) {
  case_when(
    Error_type == "SE" ~ Value * sqrt(Sample_size),
    Error_type == "SD" ~ Value,
    Error_type == "CV" ~ Value * Mean
   # Error_type == "95CI" ~ Value #issue here as the only paper that I have errors as 95CI also used LS means - cross this bridge if LS means become included
  )
}
```

Execute

```{r}
df_SMD4 <- df_SMD3 %>% 
  mutate(Con_SD = to_SD(Control_Error_type, Control_Error, `Control sample size`, `Control mean`),
         Trt_SD = to_SD(Treatment_Error_type, Treatment_Error, `Treatment sample`, `Treatment mean`))
```

#### Trim up

The output dataframes that I will use for analysis do not need to contain columns that were relevant for data extraction and identification.

I'll rename some column headings for greater clarity and then trim down the dataframe to the necessary columns.

```{r}
df_SMD5 <- df_SMD4 %>% 
  rename(Method = `Control method`, Type = `Method sub-type`, Plant_genus = `Plant genus`, Scale = `Scale (plant, coupe, landscape)`, Variable = `Var. measured`, Confounding = `Confounding factors`, Effect_type = `Effect type`, Con_N = `Control sample size`, Con_mean = `Control mean`, Trt_N = `Treatment sample`, Trt_mean = `Treatment mean`)
```

```{r}
df_SMD6 <- df_SMD5 %>% 
  select(Paper_ID, Method, Type, Animal, Plant_genus, Scale, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Con_SD, Trt_N, Trt_mean, Trt_SD, Trans, Con_trans_mean, Trt_trans_mean)
```

#### Convert and transform errors

Now I will transform the SDs for the SMD proportion data as mentioned earlier - use Trans = "Y" to quickly identify the rows for which this is relevant and then apply the functions written earlier.

```{r}
# write function for SD
logit_trans_SD <- function(SD, prop, ID){
  if_else(ID == "Y", sqrt(((SD^2)*(1/prop)) + (1/(1-(prop^2)))), NA)
}

# first convert instances where SD is still percentage rather than proportion

df_SMD_sdtrans <- df_SMD6 %>% mutate(Con_trans_SD = pct_to_prop(Units, Con_SD),
                                     Trt_trans_SD = pct_to_prop(Units, Trt_SD))

#then execute logit function

df_SMD_sdtrans1 <- df_SMD_sdtrans %>% mutate(Con_trans_SD = logit_trans_SD(Con_SD, Con_mean, Trans), 
                                             Trt_trans_SD = logit_trans_SD(Trt_SD, Trt_mean, Trans))
```

Write this dataframe out as a record of the Complete case data showing both raw means and sds and transformed means and sds

```{r}
write_xlsx(df_SMD_sdtrans1, "/homevol/jbhg/Browse_meta/Data processing/SMD_compcase_raw and transform.xlsx")
```

#### Quality control 1

Need to impute the errors for cases where no error was reported - this is handled later.

I need to assess the quality of the data that will be used for the imputation and further downstream analyses. Instances where the control/treatment SD is 0 will not be suitable for generating SMD effect sizes as they cause an effect size of infinity. Moreover, instances where either sample size is 1 will result in an SD = 0 creating the same problem. I will remove these cases and then conduct a modified Geary's test (Lajeunesse 2015, Nakagawa et al 2023).

Remove problematic data

```{r}
# Filter out instances where control/treatment SD = 0
df_SMD7 <- df_SMD_sdtrans1 %>% 
  filter(Con_SD != 0 & Trt_SD != 0)

# Filter out instances where control/treatment N = 1 
df_SMD8 <-  df_SMD7 %>% 
  filter(Con_N != 1 & Trt_N != 1)

#filter out transformed data that was problematic ie produced an infinite value
df_SMD9 <- df_SMD8 %>% 
  filter(Con_trans_mean != Inf | is.na(Con_trans_mean)) %>% filter(Trt_trans_mean != Inf | is.na(Trt_trans_mean)) %>%
  filter(Con_trans_SD != Inf | is.na(Con_trans_SD)) %>% filter(Trt_trans_SD != Inf | is.na(Trt_trans_SD))
```

### Complete case data generation

I'm going to conduct a complete case and imputed analysis.

First ensure that instances with transformed data are using the transformed data to generate effect sizes.

```{r}
df_SMD10 <- df_SMD9 %>% mutate(Con_mean = if_else(is.na(Trans), Con_mean, Con_trans_mean),
                              Con_SD = if_else(is.na(Trans), Con_SD, Con_trans_SD),
                              Trt_mean = if_else(is.na(Trans), Trt_mean, Trt_trans_mean),
                              Trt_SD = if_else(is.na(Trans), Trt_SD, Trt_trans_SD))
```

Take the complete case data and generate SMD effect sizes using metafor - escalc()

```{r}
df_comp_SMD <- df_SMD9 %>% 
  mutate(Effect_size = escalc("SMD", m1i = Trt_mean, sd1i = Trt_SD, n1i = Trt_N, m2i = Con_mean, sd2i = Con_SD, n2i = Con_N,     data = df_SMD9, var.names = c("Effect_size", "Sampling_variance"), append = FALSE))
```

Coin effects

```{r}
df_SMD_var <- df_SMD_all2 %>% 
  distinct(Variable)
```

Write the function

```{r}
coin_SMD <- function(Variable, Value) {
  case_when(Variable == "sqrt Height removed" | Variable == "Bark wounds/tree" | Variable == "Branches browsed" | 
            Variable == "Browse intensity" | Variable == "Browsing" | Variable == "Browsing damage" | Variable == "Consumption" |
            Variable == "Damage score" | Variable == "Deformed base" | Variable == "Deformed stem" | Variable == "Foliage removed" |
            Variable == "Height removed" | Variable == "Height to 1st branch" | Variable == "Juveniles browsed" |
            Variable == "Lateral shoot browsing" | Variable == "Leader browsing" | Variable == "Leaves eaten" | 
            Variable == "log(twigs|stems browsed)" | Variable == "Mass of browsed seedlings" | Variable == "Mortality" |
            Variable == "Multi stem plants" | Variable == "No. of leaders" | Variable == "Plants browsed" |
            Variable == "Prop. bottom consumed" | Variable == "Prop. top consumed" | Variable == "Shoots browsed" |
            Variable == "Shoots eaten/plant" | Variable == "Stems damaged" | Variable == "Suckers browsed" |
            Variable == "Twigs removed/tree" | Variable == "Twigs/tree browsed" | Variable == "Twigs|Stems browsed" ~ Value * -1,
            TRUE  ~ Value)
}
```

Execute - need to unnest the Effect_size columns first (escalc class objects don't work for these functions)

```{r}
df_comp_SMD1 <- df_comp_SMD %>% unnest_wider(Effect_size) %>% 
  mutate(Effect_size = coin_SMD(Variable, Effect_size))
```


Write out comp case SMD only effects data

```{r}
write_xlsx(df_comp_SMD1, "/homevol/jbhg/Browse_meta/Data processing/SMD_compcase_es.xlsx")
```

### Imputed case data generation

Perform modified Geary's test

```{r}
# Function to calculate Geary's "number"
  geary <- function(mean, sd, n){
    (1 / (sd / mean)) * ((4*n^(3/2)) / (1 + 4*n))
  }

#Perform test
df_SMD_geary <- df_SMD8 %>% 
  mutate(Con_geary = geary(Con_mean, Con_SD, Con_N),
         Trt_geary = geary(Trt_mean, Trt_SD, Trt_N),
         Geary_test = ifelse(Con_geary >=3 & Trt_geary >= 3, "Pass", "Fail"))

Geary_results <- df_SMD_geary %>% group_by(Geary_test) %>% summarise(n = n())
```

Approximately 32% of effects fail the Geary test.

I need to rebind the missing cases to the full cases. To do that I need to make sure the 2 dataframes have the same structure.

```{r}
df_SMD_miss1 <- df_SMD_miss %>%
  rename(Method = `Control method`, Type = `Method sub-type`, Plant_genus = `Plant genus`, Scale = `Scale (plant, coupe, landscape)`, Variable = `Var. measured`,
         Confounding = `Confounding factors`, Effect_type = `Effect type`, Con_N = `Control sample size`, Con_mean = `Control mean`, Trt_N = `Treatment sample`, 
         Trt_mean = `Treatment mean`) %>% 
  mutate(Con_SD = NA,
         Trt_SD = NA) %>% 
  select(Paper_ID, Method, Type, Animal, Plant_genus, Scale, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Con_SD, Trt_N, Trt_mean, Trt_SD, Trans, Con_trans_mean, Trt_trans_mean)
  
```

Join them up

```{r}
df_SMD_all <- bind_rows(df_SMD9, df_SMD_miss1)
```

Write this out as a record of all cases with indication of transformed data

```{r}
write_xlsx(df_SMD_all, "/homevol/jbhg/Browse_meta/Data processing/SMD_impcase_raw and transform.xlsx")
```

Ensure correct means are being used

```{r}
df_SMD_all1 <- df_SMD_all %>% mutate(Con_mean = if_else(is.na(Trans), Con_mean, Con_trans_mean),
                              Con_SD = if_else(is.na(Trans), Con_SD, Con_trans_SD),
                              Trt_mean = if_else(is.na(Trans), Trt_mean, Trt_trans_mean),
                              Trt_SD = if_else(is.na(Trans), Trt_SD, Trt_trans_SD)) %>% 
  select(Paper_ID, Method, Type, Animal, Plant_genus, Scale, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Con_SD, Trt_N, Trt_mean, Trt_SD, Trans)
```

Run multiple imputation to fill in the missing standard deviations.

```{r}
#DN: I'll note some odd missing data patterns where you're missing trt and control means and N. I think if you're missing means I'd probably just exclude. I would probably limit imputation to situations where you: 1) have N and 2) have means, but are missing SDs OR 1) have SD, SE and 2) have means. I think the missing data patterns in there that you have at times are not really conducive to imputation. 
md.pattern(df_SMD_all1, rotate.names = TRUE) 
aggr(df_SMD_all1)

# Usually there is a strong mean-variance relationship. This should help you impute missing data well. We can plot these to have a look. 

p1 <- ggplot(df_SMD_all1, aes(x = log(Con_mean), y = log(Con_SD))) + geom_point() + geom_smooth(method = "lm", se = FALSE) + labs(title = "Control", y = "log(SD)", x = "log(Mean)")
p2 <- ggplot(df_SMD_all1, aes(x = log(Trt_mean), y = log(Trt_SD))) + geom_point() + geom_smooth(method = "lm", se = FALSE) + labs(title = "Treatment", y = "log(SD)", x = "log(Mean)")

p1/p2

# Yes, clear mean variance relationship. This should help with imputation, but there are some issues with the data. Probably a lot of 'zeros', which will cause problems but not necessarily incorrect. Though, SD = 0? Also possible I suppose.

df_SMD_all2 <- df_SMD_all1 %>% 
  mutate(log_Con_mean = log(Con_mean),
         log_Con_SD = log(Con_SD),
         log_Trt_mean = log(Trt_mean),
         log_Trt_SD = log(Trt_SD))

# on review of some the data there are some instances where the log mean is minus infinity - will need to track these back to their papers

# Create an indicator column showing which rows have been imputed vs which had complete data
df_SMD_all3 <- df_SMD_all2 %>% mutate(Imputed = if_else(is.na(Con_SD), "Y", "N"))
```

#### Quality control

Obvious issues here with data points that the log scale is putting at infinity - instances where the mean is 0 and hence the standard deviation should also be 0. These will be excluded from the imputation process to ensure as robust as possible imputation. Note that this may create some disparity between complete case and imputed case where complete case has effects not included in the imputed case.

```{r}
df_SMD_all4 <- df_SMD_all3 %>% filter(is.finite(log_Con_mean))
```


```{r}
# Make predictor matrix which defines which variables will be used to impute. Should moderators be included in this or just use thete numeric data alone? DN: technically you can include as many moderators as you can to help with imputation. paperID is probably sensible because effects from teh same paper will have the same sample sizes
predmatHg <- make.predictorMatrix(df_SMD_all3)
predmatHg[, c(2:13,24,25)] <- 0
predmatHg[c(1:15,17,18,20,22,24,25), ] <- 0

#Ensure missing log data is only imputed using log data and vice versa for original data
predmatHg[16, 20:25] <- 0
predmatHg[19, 20:25] <- 0
predmatHg[21, c(15,16,18,19)] <- 0
predmatHg[23, c(15,16,18,19)] <- 0

# How well does pmm performs? - had to write out the meth vector in full to stop it imputing the months: DN: You shouldn't need to. I think it had to do with some issues in your prediction matrix
impData_SMD <- mice(df_SMD_all3, m=20, maxit=50, meth = c("","","","","","","","","","","","","","","","pmm","","","pmm","","pmm","","pmm","",""), pred = predmatHg, seed=500)
summary(impData_SMD)
```

```{r}
# Looking at these there are 2 papers with very large means and errors that are making the plots difficult to interpret (Kuiters et al and Leonardsson et al)
mice::xyplot(impData_SMD, Con_SD ~ Con_mean)
mice::xyplot(impData_SMD, Trt_SD ~ Trt_mean, xlim = c(0,20000), ylim = c(0,20000))
mice::xyplot(impData_SMD, log_Con_SD ~ log_Con_mean)
mice::xyplot(impData_SMD, log_Trt_SD ~ log_Trt_mean)
# mice::densityplot(impData_SMD) #this one isn't working - not entirely sure why? when the months are left in this plots out fine

# Get all imputed datasets
SMD_datasets <- complete(impData_SMD, "all")

```

Need to back transform the imputed log data.

```{r}
SMD_datasets1 <- lapply(SMD_datasets, function(df){
  df %>% mutate(Trans_Con_mean = exp(log_Con_mean),
                Trans_Con_SD = exp(log_Con_SD),
                Trans_Trt_mean = exp(log_Trt_mean),
                Trans_Trt_SD = exp(log_Trt_SD)) 
  })
```

Use metafor package escalc() to generate the effect sizes. Will then need to review the variables that were measured in order to make sure that instances where a negative impact on the variable is a positive browsing outcome are accounted for.

Conventional SMD effects - conducted on both the data imputed from the raw data and the data imputed from the log transformed data

```{r}
SMD_imp_raw <- lapply(SMD_datasets1, function(df){
  df %>% mutate(Effect_size = escalc("SMD", m1i = Trt_mean, sd1i = Trt_SD, n1i = Trt_N, m2i = Con_mean, sd2i = Con_SD, n2i = Con_N, data = df,
                                     var.names = c("Effect_size", "Sampling_variance"), append = FALSE))
 })

SMD_imp_log <- lapply(SMD_datasets1, function(df){
  df %>% mutate(Effect_size = escalc("SMD", m1i = Trans_Trt_mean, sd1i = Trans_Trt_SD, n1i = Trt_N, m2i = Trans_Con_mean, sd2i = Trans_Con_SD, n2i = Con_N,
                                     data = df, var.names = c("Effect_size", "Sampling_variance"), append = FALSE))
 })
```

Getting some warnings here - could be instances where the SD is 0 (may need to add a constant to fix those or review those cases and exclude) - will fix later. Have elected to exclude instances with a SD of 0

Coining

```{r}
SMD_imp_log1 <- lapply(SMD_imp_log, function(df){
  df %>% unnest_wider(Effect_size) %>% 
  mutate(Effect_size = coin_SMD(Variable, Effect_size))
})

SMD_imp_raw1 <- lapply(SMD_imp_raw, function(df){
  df %>% unnest_wider(Effect_size) %>% 
  mutate(Effect_size = coin_SMD(Variable, Effect_size))
})
```

Save lists of imputed dataframes for later analysis.

```{r}
saveRDS(SMD_imp_log1, "/homevol/jbhg/Browse_meta/Data processing/SMD_imp_data_log")

saveRDS(SMD_imp_raw1, "/homevol/jbhg/Browse_meta/Data processing/SMD_imp_data_raw")

# We can then sample over all these datasets in a Bayseian model or use Rubin's rules to pool estimates. DN: note that rubins rules assume you're ruing metafor::rma.mv for the model. 
```

## Log odds effects

### Trim up

Clean up the dataframe and rename some columns for easier downstream processing.

```{r}
df_lo1 <- df_lo %>% 
  rename(Method = `Control method`, Type = `Method sub-type`, Plant_genus = `Plant genus`, Scale = `Scale (plant, coupe, landscape)`, Variable = `Var. measured`, Confounding = `Confounding factors`, Effect_type = `Effect type`, Con_N = `Control sample size`, Con_mean = `Control mean`, Trt_N = `Treatment sample`, Trt_mean = `Treatment mean`)
```

```{r}
df_lo2 <- df_lo1 %>% 
  select(Paper_ID, Method, Type, Animal, Plant_genus, Scale, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_mean, Trt_N, Trt_mean)
```

### Count data

The binomial data I have eg survival or browsed/not browsed has been measured with various units: percent, proportion, probability, count etc. I need to get all of the log odds effects back to count data so that I can generate effect sizes. I need to write a function to convert the values based on the units. Then I can generate the effect sizes.

What units need to be accounted for? Make any edits for entry errors

```{r}
df_lo_units <- df_lo2 %>% 
  distinct(Units)

df_lo2 <-  df_lo2 %>% 
  mutate(Units = ifelse(Units == "count", "Count", Units))
```

Write the function

```{r}
to_count <- function(Unit, Value, Sample_size) {
  counts <- case_when(
    Unit == "Count" ~ Value,
    Unit == "Percent" ~ (Sample_size/100) * Value,
    Unit ==  "Proportion" ~ Sample_size * Value,
    Unit ==  "Probability" ~ Sample_size * Value)
  
  output <- round(counts, 0) #count data has to be whole number - survival etc can't be half a plant survived
  
  return(output)
} 
```

Execute and trim

```{r}
df_lo3 <- df_lo2 %>% 
  mutate(Con_count = to_count(Units, Con_mean, Con_N),
         Trt_count = to_count(Units, Trt_mean, Trt_N)) %>% 
  select(Paper_ID, Method, Type, Animal, Plant_genus, Scale, Months, Variable, Units, Confounding, Effect_type, Shared_data, Repeat_measures, Con_N, Con_count, Trt_N, Trt_count)
```

### Generate effect sizes

Using the count data I can generate the log odds effect sizes ready for analysis. I will need to account for the fact that some of the variables measured act in opposite directions when there has been a positive outcome eg survival vs mortality. I should be able to account for this by looking at the variables measured and switching the signs of the relevant data points.

Use metafor escalc() to calculate the log odds ratio effect sizes.

```{r}
df_lo3$Effect_size <- escalc(measure = "OR", ai = Trt_count, n1i = Trt_N, ci = Con_count, n2i = Con_N, data = df_lo3,                                       var.names = c("Effect_size", "Sampling_variance"), append = FALSE)
```

### Coining

Now I need to investigate the variables that have been measured across studies and account for instances where a negative relationship is actually a positive outcome eg lower mortality than control group is a positive browsing outcome. These instances will need to be flipped so that the effect is acting in the correct direction.

```{r}
df_lo_var <- df_lo3 %>% 
  distinct(Variable)
```

Write a function for the sign switching

```{r}
coin_lo <- function(Variable, Value) {
  case_when(Variable == "Mortality" | Variable == "Plants damaged" | Variable == "Plants browsed" | Variable == "Leader browsing" |
            Variable == "Lateral browsing" | Variable == "Bud damage" | Variable == "Leaf damage" | Variable == "Stems harvested" 
            ~ Value * -1,
            TRUE ~ Value)
            
}
```

Execute

```{r}
df_lo4 <- df_lo3 %>% unnest_wider(Effect_size) %>% 
  mutate(Effect_size = coin_lo(Variable, Effect_size))
```

Write out the file with the generated effects ready for analysis

```{r}
write_xlsx(df_lo4, "Meta_lo_es.xlsx") #add metadata sheet to this file
```

## All effects comparison

Need to convert the log odds effects into something that can be readily compared to the SMD effects and then rejoin the dataframes into an all cases df.

### Conversion

Write the functions for converting from Log odds to SMD - using formulae from Borenstein chapter in Cooper et al. (2019) Handbook of meta-analysis

```{r}
lo_to_SMD_ES <- function(Lo_es){
  SMD_es <- (Lo_es*sqrt(3)) / pi 
  
  return(SMD_es)
}

lo_to_SMD_var <- function(Lo_var){
  SMD_var <- (Lo_var*3) / (pi^2) 
  
  return(SMD_var)
}
```

Execute those functions

```{r}
df_lo_conv <- df_lo4 %>% mutate(
  Effect_size = lo_to_SMD_ES(Effect_size),
  Sampling_variance = lo_to_SMD_var(Sampling_variance)
)
```

### Rejoin

```{r}
df_comp_all <- bind_rows(df_comp_SMD1, df_lo_conv) 
  
df_imp_all <- bind_rows()
```

### Split into different scales

```{r}
df_comp_plant <- df_comp_all %>% filter(Scale == "Plant")
write_xlsx(df_comp_plant, "/homevol/jbhg/Browse_meta/Analysis/Comp_comb_plant.xlsx")
  
df_comp_coupe <- df_comp_all %>% filter(Scale == "Coupe")
write_xlsx(df_comp_coupe, "/homevol/jbhg/Browse_meta/Analysis/Comp_comb_coupe.xlsx")
  
df_comp_lscape <- df_comp_all %>% filter(Scale == "Landscape")
write_xlsx(df_comp_lscape, "/homevol/jbhg/Browse_meta/Analysis/Comp_comb_lscape.xlsx")
```

```{r}
df_imp_plant <- 
  
df_imp_coupe <- 
  
df_imp_lscape <-
```
